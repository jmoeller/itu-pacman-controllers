\documentclass[conference]{IEEEtran}
% If the IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it.  e.g.
% \documentclass[conference]{./IEEEtran}

% Add and required packages here
\usepackage{graphicx,times,amsmath}
\usepackage[utf8]{inputenc} 

% Correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor IEEEtran}

% To create the author's affliation portion using \thanks
\IEEEoverridecommandlockouts

\textwidth 178mm
\textheight 239mm
\oddsidemargin -7mm
\evensidemargin -7mm
\topmargin -6mm
\columnsep 5mm

\begin{document}

% Project title: keep the \ \\ \LARGE\bf in it to leave enough margin.
\title{\ \\ \LARGE\bf Development of AI Ms. Pac-Man Controllers}

\author{Julian MÃ¸ller (jumo@itu.dk)}

% Uncomment out the following line for invited papers
\specialpapernotice{Modern AI for Games, Fall Semester 2013, IT-University of Copenhagen}

% Make the title area
\maketitle

\begin{abstract}
In this paper, successful implementations of two Ms. Pac-Man controllers and unsuccessful implementation of one Ms. Pac-Man controller is described. The methodologies used are genetic evolution of state machines, Monte Carlo Tree Search for Microplanning and Neural Network with multilayered perceptron.
The results of the implementations as well as their shortcomings are described. Comparisons are be made with the benchmark controllers
\emph{StarterGhosts}, \emph{Legacy} and \emph{Legacy2TheReckoning}. Maximum scores of approximately 14.000 (with means around 6.500 points)
are obtained against the toughest ghosts.
\end{abstract}


\section{Introduction}
In this paper I will describe the controllers I have developed for the real-time arcade game Ms. Pac-Man (henceforth named Pac-Man).

I will first describe the state based controller which was evolved via genetic programming. Then, I will describe the Monte Carlo Tree
Search controller and lastly, I will describe the controller backed by a multilayer perceptron network. I will then present my results. To end with, I will discuss and evaluate the results as well as the project in general.


\section{Genetically Evolved State Machine}
\input{fsm_evo}

\section{Monte Carlo Tree Search}
\input{mcts}

\section{Multilayered Perceptron Network}
\input{mlp.tex}


\section{Results}

\subsection{Evolution of genomes for the SMC}

Figures \ref{fig-evos}, \ref{fig-evol} and \ref{fig-evol2} show the average score of the controller as the evolution progressed. The controller reached a quite stable score against all three ghosts. The StarterGhosts were more forgiving with regards to genome parameters than the Legacy and Legacy2 ghosts, as the average score was quite high to begin with. For all three simulations, a 'maximum' fitness was reached quite early.

\begin{figure}[htp]
\centerline{\includegraphics[width=0.9\columnwidth]{s_evo.png}}
\caption{Evolution vs. StarterGhosts}
\label{fig-evos}
\end{figure}

\begin{figure}[htp]
\centerline{\includegraphics[width=0.9\columnwidth]{l_evo.png}}
\caption{Evolution vs. Legacy}
\label{fig-evol}
\end{figure}

\begin{figure}[htp]
\centerline{\includegraphics[width=0.9\columnwidth]{l2_evo.png}}
\caption{Evolution vs. Legacy2TheReckoning}
\label{fig-evol2}
\end{figure}

\subsection{Learning rate for the neural network}

Figure \ref{fig-learnrate} show the mean squared error for the different learning rates $0.2$ and $0.5$. Both errors stabilize very fast, but at different levels. That the error is lower for a lower learning rate is to be expected. 

\begin{figure}[htp]
\centerline{\includegraphics[width=0.9\columnwidth]{learning_rate.png}}
\caption{Mean, squared error for different learning rates}
\label{fig-learnrate}
\end{figure}

\subsection{Performance against ghosts}

Figure \ref{fig-avgs} shows the performance against the starter ghosts. \emph{S}, \emph{L} and \emph{L2} represent that the values for the genome are the ones found to be best against StarterGhosts, Legacy and Legacy2TheReckoning, respectively.

The most stable performer is the MCTS controller, but due to its micro- instead of macro-strategy, it doesn't score very high. All three genomes for the state machine performs better than StarterPacMan, but both StarterPacMan and all three state machines have a quite large standard deviance. The L2 version performs about as well as the version evolved against the ghosts specifically, but has a higher standard deviation.

Figure \ref{fig-avgl} show the same pattern: The controllers perform about the same, with an advantage for the SMC that was specifically evolved to handle the Legacy ghosts.

Another picture is painted in figure \ref{fig-avgl2}, where the SMC specifically evolved show a much higher average score than the rest of the controllers. I believe this can both be because the ghosts have a very distinct behavior, but also because the ghosts are less forgiving, so a specialized version will perform better.

Figure \ref{fig-cvals} show the average scores for the MCTS controller based on different values for the C-component, that determine the rate of exploration vs exploitation when the search tree is traversed. The most solid value is $C = 0.2$, which displays the lowest variance of all of my controllers, but an optimal C-value requires more extensive testing than I have performed.\footnote{Because the MCTS utilized all available time given, the games were very slow, so it wasn't feasible to run that many simulations.}

\begin{figure}[htp]
\centerline{\includegraphics[width=0.9\columnwidth]{average_starter.png}}
\caption{Average score against StarterGhosts}
\label{fig-avgs}
\end{figure}

\begin{figure}[htp]
\centerline{\includegraphics[width=0.9\columnwidth]{average_legacy.png}}
\caption{Average score against Legacy}
\label{fig-avgl}
\end{figure}

\begin{figure}[htp]
\centerline{\includegraphics[width=0.9\columnwidth]{average_legacy2.png}}
\caption{Average score against Legacy2TheReckoning}
\label{fig-avgl2}
\end{figure}

\begin{figure}[htp]
\centerline{\includegraphics[width=0.9\columnwidth]{c_values.png}}
\caption{Average score against StarterGhosts for different C-values}
\label{fig-cvals}
\end{figure}

\section{Conclusions}

I set out to create a SMC that performed better than the starter ghosts, which I did, so I am happy about that.
I would have liked to create a macro-based MCTS controller instead of the micro-based one I implemented, to score better, but I am happy with the high stability and low standard deviance the controller exhibited.

I am not satisfied, that I couldn't get the MLP network to work, as that is the solution I think is the most interesting. I underestimated how long time it could take to debug the MLP implementation. I also wasted too much time on improving the state based controller, that I should have used on the MLP and on making the MCTS better.

% Trigger a \newpage just before a given reference number in order to
% balance the columns on the last page.  Adjust the value as needed;
% it may need to be readjusted if the document is modified later.
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% The references section can either be generated by hand or by an
% automatic tool like BibTeX.  If using BibTex, use the standard IEEEtran
% bibliography style.
%\bibliographystyle{IEEEtran.bst}
%
% The argument to \bibliography is/are the name(s) of your BibTeX file(s)
% that contains string definitions and bibliography database(s).
%\bibliography{IEEEabrv,SamplePaper}
%
% If you generate the bibliography by hand, or if you copy in the
% resultant .bbl file, set the second argument of \begin to the number of
% references in the bibliography (used to reserve space for the reference
% number labels box).

\begin{thebibliography}{3}
\bibitem{aima}
S.~Russel, P.~Norvig \emph{Artificial Intelligence - A Modern Approach}.\hskip 1em plus 0.5em minus 0.4em\relax
  Pearson, 2010.


\bibitem{enhancements}
T.~Pepels, M.H.M.~Winands, ``Enhancements for Monte-Carlo Tree Search in Ms Pac-Man,''


\bibitem{datamining}
J. Han, M. Kamber, and J. Pei. \emph{Data Mining: Concepts and Techniques (3rd ed.)}, 2011.\hskip 1em plus 0.5em minus 0.4em\relax
Morgan Kaufmann Publishers Inc.

\end{thebibliography}

\end{document}
