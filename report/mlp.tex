My implementation of this controller succeeded. But the behaviour generated by the controller was not successful.
I created a neural network that successfully learned various boolean functions\footnote{AND, OR, NOT, NOR, XOR, T, F}.

However, when I tried to train the network to play Pac-Man based on data recordings of myself, nothing really happened. Because I got the neural network working very late, I didn't have time to find the errors. I also didn't have time to network run for longer periods to see if the problem was merely too short a maximum training time\footnote{Which I had at 15 minutes}.

So, this is a description of what I did, which unfortunately won't be accompanied by any results.

Multilayered perceptron networks is a machine learning technique, where a number of inputs are passed through a network of sums, weights and functions that map the values onto the $[0;1]$ domain. The result is a single or a number of numbers, that (hopefully) represent some learned knowledge.

This can for instance be utilized in Pac-Man to determine a 'goodness' score for a given state, and by feeding the neural network each possible move as a distinct state, determine which is the better action to perform. A neural network can also be used to determine which state it is favorable for Pac-Man to be in (such as flee, eat pill, seek power pill or hunt ghost).

A general problem with a MLP network is determining the topology of the network, as there is no known way to determine beforehand if a topology will be a good fit for a certain problem space. This could be 'solved' by utilizing genetic programming and letting an algorithm evolve the topology based on the achieved scores in the game.

Another problem is that MLP networks suffer from overfitting, which requires multiple distinct sets of data for training, test and verification. To get consistent results, this requires a consistent controller used for training.

\subsection*{Implementation}

The neural network is implemented directly based on the algorithms found in \cite{aima} and \cite{datamining}.

I tried to make the neural network have four outputs, each representing a direction that Pac-Man can take. The inputs are everything that is available from the supplied \texttt{DataTuple} class:

\begin{itemize}
\item Distance to every one of the four ghosts
\item Whether each of the four ghosts are edible
\item The number of pills left
\item The number of power pills left
\item The current score
\item The current level time
\item The total game time
\end{itemize}

This represented my input layer. Left, is the hidden layer which for number of nodes had the mean of the number of input nodes and number of output nodes.

\subsection*{Primary problems}

Had I gotten the MLP controller to function at an earlier point in time, I would also have liked to explore the following possibilities for input values:

\begin{itemize}
\item Is Pac-Man at a junction?
\item Current score
\item Number of ghosts eaten
\item Is (the nearest) ghost ahead of or behind Pac-Man
\item Where in the maze is Pac-Man?
\end{itemize}

I would also have liked to run the training for longer periods of time, to see if my problems would solve themselves, if the training process was merely slow.

Yet another problem could possibly be my topology, more specifically the number of output nodes. I also considered having the network try to classify the "goodness" of a state, instead of having it try to predict the road chosen with four different outputs. The network would have been fed each of the possible game states, \emph{after} a Pac-Man move had been simulated. The different scores from the network would then have been used to determine the best action.